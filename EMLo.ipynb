{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadData():\n",
    "    '''\n",
    "    The functions include load data\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''Initialize kinds of parameters，adjust the parameters accroding to data'''\n",
    "        self.seq_length = 128\n",
    "        \n",
    "    def load_directory_data(self, directory):\n",
    "        '''Load all files from a directory in a DataFrame.'''\n",
    "        data = {}\n",
    "        data[\"sentence\"] = []\n",
    "        data[\"sentiment\"] = []\n",
    "        for file_path in os.listdir(directory):\n",
    "            with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "                data[\"sentence\"].append(f.read())\n",
    "                data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "        return pd.DataFrame.from_dict(data)\n",
    "\n",
    "    def load_dataset(self, directory):\n",
    "        '''Merge positive and negative examples, add a polarity column and shuffle'''\n",
    "        pos_df = self.load_directory_data(os.path.join(directory, \"pos\"))\n",
    "        neg_df = self.load_directory_data(os.path.join(directory, \"neg\"))\n",
    "        pos_df[\"polarity\"] = 1\n",
    "        neg_df[\"polarity\"] = 0\n",
    "        return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    def download_and_load_datasets(self, force_download=False):\n",
    "        '''加载斯坦福提供的英文文本分类训练数据'''\n",
    "        dataset = tf.keras.utils.get_file(\n",
    "            fname=\"aclImdb.tar.gz\", \n",
    "            origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
    "            extract=True)\n",
    "\n",
    "        train_df = self.load_dataset(os.path.join(os.path.dirname(dataset), \"aclImdb\", \"train\"))\n",
    "        test_df = self.load_dataset(os.path.join(os.path.dirname(dataset), \"aclImdb\", \"test\"))\n",
    "        return train_df, test_df\n",
    "\n",
    "    def get_train_test_data(self, train_df, test_df):\n",
    "        '''\n",
    "        将数据有DataFrame转化为可用的np.array形式\n",
    "        Args:\n",
    "            train_df:训练数据DataFrame\n",
    "            test_df:测试数据DataFrame\n",
    "        Return:\n",
    "            train_data = [train_text, train_label, train_label_one_hot]\n",
    "            test_data = [test_text, test_label, test_label_one_hot]\n",
    "        '''\n",
    "        # Create datasets (Only take up to 150 words for memory)\n",
    "        train_text = train_df['sentence'].tolist()\n",
    "        train_text = [' '.join(t.split()[0:self.seq_length]) for t in train_text]\n",
    "        train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
    "        train_label = train_df['polarity'].tolist()\n",
    "        train_label_one_hot = [[1, 0] if train_label[i]==0 else [0, 1] for i in range(len(train_label))]\n",
    "\n",
    "        test_text = test_df['sentence'].tolist()\n",
    "        test_text = [' '.join(t.split()[0:self.seq_length]) for t in test_text]\n",
    "        test_text = np.array(test_text, dtype=object)[:, np.newaxis]\n",
    "        test_label = test_df['polarity'].tolist()\n",
    "        test_label_one_hot = [[1, 0] if test_label[i]==0 else [0, 1] for i in range(len(test_label))]\n",
    "        train_data = [train_text, train_label, train_label_one_hot]\n",
    "        test_data = [test_text, test_label, test_label_one_hot]\n",
    "        return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElmoModel_for_Classification():\n",
    "    '''\n",
    "    This class is created by zhanglei at 2019/06/10.\n",
    "    The environment: python3.5 or later and tensorflow1.10 or later.\n",
    "    The functions include set parameters，build elmo model + two lstm for classification.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialize kinds of parameters，adjust the parameters accroding to data\n",
    "        '''\n",
    "        self.batch_size = 32          #训练batch大小\n",
    "        self.seq_length = 128         #序列长度\n",
    "        self.embedding_size = 1024    #embedding大小，elmo模型是固定的1024\n",
    "        self.hidden_size = 128        #隐层神经元个数\n",
    "        self.class_num = 2            #分类类别数，二分类\n",
    "        self.keep_prob = 0.5          #防止过拟合\n",
    "        self.num_layers = 2           #lstm层数\n",
    "        self.learning_rate = 1e-3     #学习率大小\n",
    "        self.elmo_model()\n",
    "\n",
    "    def elmo_model(self):\n",
    "        '''\n",
    "        build model\n",
    "        '''\n",
    "        tf.reset_default_graph()\n",
    "        self.input_x = tf.placeholder(tf.string, [None, 1], name='input_x')\n",
    "        self.input_y = tf.placeholder(tf.int32, [None, 2], name='input_y')\n",
    "        self.global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "        \n",
    "        self.elmo = hub.Module('https://tfhub.dev/google/elmo/2', trainable=True)\n",
    "        self.embedding = self.elmo(tf.squeeze(tf.cast(self.input_x, tf.string), axis=1), as_dict=True, signature='default')\n",
    "        i\n",
    "        # LstmCell单元的隐层数取决于上一层embedding_size的大小\n",
    "        with tf.name_scope('rnn'):\n",
    "            cell = tf.contrib.rnn.BasicLSTMCell(self.embedding_size, state_is_tuple=True)\n",
    "            cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob)\n",
    "            cells = [cell for _ in range(self.num_layers)]\n",
    "            rnn_cells = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "            # 下面不使用dtype还会报错，需要初始化\n",
    "            outputs, _ = tf.nn.dynamic_rnn(cell=rnn_cells, inputs=self.embedding['elmo'], dtype=tf.float32)\n",
    "            last_outputs = outputs[:, -1, :]\n",
    "        \n",
    "        with tf.name_scope('hidden'):\n",
    "            fc = tf.layers.dense(last_outputs, self.hidden_size, name='fc1')\n",
    "            fc = tf.contrib.layers.dropout(fc, self.keep_prob)\n",
    "            fc = tf.nn.relu(fc)\n",
    "        \n",
    "        with tf.name_scope('logits'):\n",
    "            # tf.math.argmax\n",
    "            self.logits = tf.layers.dense(fc, self.class_num, name='fc2')\n",
    "            self.y_pred_cls = tf.arg_max(tf.nn.softmax(self.logits), 1)\n",
    "        \n",
    "        with tf.name_scope(\"optimize\"):\n",
    "            # 损失函数，交叉熵\n",
    "            # tf.nn.softmax_cross_entropy_with_logits_v2\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(cross_entropy)\n",
    "            self.optim = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(self.loss, global_step=self.global_step)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data, test_data, loadData, elmoModel):\n",
    "    '''\n",
    "    启动图进行模型训练\n",
    "    Args:\n",
    "        train_data:训练数据np.array\n",
    "        test_data:测试数据np.array\n",
    "        loadData:数据加载类对象\n",
    "        elmoModel:模型加载类对象\n",
    "    Return\n",
    "    '''\n",
    "    train_text, train_label, train_label_one_hot = train_data\n",
    "    test_text, test_label, test_label_one_hot = test_data\n",
    "    data_size = len(train_label_one_hot)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        num_batch_every_epoch = math.ceil(data_size/elmoModel.batch_size)\n",
    "        y_pred = np.zeros(shape=data_size, dtype=np.int32)\n",
    "        print(num_batch_every_epoch)\n",
    "        #开始迭代训练\n",
    "        for i in range(num_batch_every_epoch):\n",
    "            start_idx, end_idx = elmoModel.batch_size*i, elmoModel.batch_size*(i+1)\n",
    "            if end_idx > data_size:\n",
    "                end_idx = data_size\n",
    "                act_end_id = elmoModel.batch_size - end_idx + data_size\n",
    "                batch_y_pred = batch_y_pred[:act_end_id]\n",
    "            batch_train_text = train_text[start_idx:end_idx]\n",
    "            batch_train_label = train_label_one_hot[start_idx:end_idx]\n",
    "            train_loss, _, batch_y_pred = sess.run([elmoModel.loss, elmoModel.optim, elmoModel.y_pred_cls],\n",
    "                                feed_dict={elmoModel.input_x:batch_train_text, elmoModel.input_y:batch_train_label})\n",
    "            print(train_loss)\n",
    "            y_pred[start_idx:end_idx] = batch_y_pred \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadData = LoadData()\n",
    "train_df, test_df = loadData.download_and_load_datasets()\n",
    "train_data, test_data = loadData.get_train_test_data(train_df, test_df)\n",
    "elmoModel = ElmoModel_for_Classification()\n",
    "train_model(train_data, test_data, loadData, elmoModel)\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "\n",
    "\n",
    "# 使用tensorflow_hub加载elmo模型进行词向量预训练\n",
    "elmo = hub.Module('https://tfhub.dev/google/elmo/2', trainable=True)\n",
    "# 三条英文训练数据\n",
    "sentence_lists = [\"My name is Lei\", \"I am studying machine learning\", \"I am fine thanks\"]\n",
    "output = elmo(sentence_lists, as_dict=True)\n",
    "print(output)\n",
    "# 启动图运行elmo模型得出词向量\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "elmo_embedding = sess.run(output['elmo'])\n",
    "print(elmo_embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
